{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSLSM Learning Style Classification - Complete Analysis\n",
    "## Felder-Silverman Learning Style Model Implementation\n",
    "\n",
    "**Objective:** Achieve 96%+ accuracy in predicting learning styles from behavioral data\n",
    "\n",
    "**Current Status:** 85% R² (needs improvement)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('../data/training_data.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"📊 Dataset Shape: {df.shape}\")\n",
    "print(f\"\\n📈 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"📋 Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"📊 Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis\n",
    "\n",
    "### 3.1 Feature List and Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "feature_groups = {\n",
    "    'Active/Reflective': [\n",
    "        'activeModeRatio', 'questionsGenerated', 'debatesParticipated',\n",
    "        'reflectiveModeRatio', 'reflectionsWritten', 'journalEntries'\n",
    "    ],\n",
    "    'Sensing/Intuitive': [\n",
    "        'sensingModeRatio', 'simulationsCompleted', 'challengesCompleted',\n",
    "        'intuitiveModeRatio', 'conceptsExplored', 'patternsDiscovered'\n",
    "    ],\n",
    "    'Visual/Verbal': [\n",
    "        'visualModeRatio', 'diagramsViewed', 'wireframesExplored',\n",
    "        'verbalModeRatio', 'textRead', 'summariesCreated'\n",
    "    ],\n",
    "    'Sequential/Global': [\n",
    "        'sequentialModeRatio', 'stepsCompleted', 'linearNavigation',\n",
    "        'globalModeRatio', 'overviewsViewed', 'navigationJumps'\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_features = [f for group in feature_groups.values() for f in group]\n",
    "label_cols = ['activeReflective', 'sensingIntuitive', 'visualVerbal', 'sequentialGlobal']\n",
    "\n",
    "print(f\"📊 Total Features: {len(all_features)}\")\n",
    "print(f\"🎯 Target Labels: {len(label_cols)}\")\n",
    "print(f\"\\n📋 Feature Groups:\")\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"  {group}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions by group\n",
    "for group_name, features in feature_groups.items():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.suptitle(f'{group_name} Features Distribution', fontsize=16)\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        df[feature].hist(bins=30, ax=ax, edgecolor='black')\n",
    "        ax.set_title(feature)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Label Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot label distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('FSLSM Dimension Distributions', fontsize=16)\n",
    "\n",
    "for idx, label in enumerate(label_cols):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    df[label].hist(bins=23, ax=ax, edgecolor='black', range=(-11.5, 11.5))\n",
    "    ax.set_title(f'{label} (mean={df[label].mean():.2f}, std={df[label].std():.2f})')\n",
    "    ax.set_xlabel('Score (-11 to +11)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.axvline(0, color='red', linestyle='--', label='Balanced')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for each dimension\n",
    "for group_name, features in feature_groups.items():\n",
    "    # Get corresponding label\n",
    "    label_map = {\n",
    "        'Active/Reflective': 'activeReflective',\n",
    "        'Sensing/Intuitive': 'sensingIntuitive',\n",
    "        'Visual/Verbal': 'visualVerbal',\n",
    "        'Sequential/Global': 'sequentialGlobal'\n",
    "    }\n",
    "    label = label_map[group_name]\n",
    "    \n",
    "    # Calculate correlations\n",
    "    corr_data = df[features + [label]].corr()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_data, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(f'{group_name} - Feature Correlations with {label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top correlations with label\n",
    "    label_corr = corr_data[label].drop(label).sort_values(ascending=False)\n",
    "    print(f\"\\n📊 {group_name} - Top correlations with {label}:\")\n",
    "    print(label_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Labeling Algorithm Explanation\n",
    "\n",
    "### Rule-Based Labeling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\n🎯 LABELING ALGORITHM EXPLANATION\\n\" + \"=\"*60)\n",
    "\n",
    "The synthetic data is generated using a rule-based algorithm based on FSLSM theory:\n",
    "\n",
    "1. GENERATE LEARNING STYLE PROFILE (Labels)\n",
    "   - Randomly generate scores for each dimension: -11 to +11\n",
    "   - These represent the \"true\" learning style\n",
    "\n",
    "2. GENERATE BEHAVIORAL FEATURES (Based on Profile)\n",
    "   For each dimension, features are generated to match the profile:\n",
    "   \n",
    "   Active/Reflective:\n",
    "   - If Active (score < -3): High activeModeRatio (0.6-0.9), many questions/debates\n",
    "   - If Reflective (score > 3): High reflectiveModeRatio (0.6-0.9), many reflections/journals\n",
    "   - If Balanced: Moderate values for both\n",
    "   \n",
    "   Sensing/Intuitive:\n",
    "   - If Sensing (score < -3): High sensingModeRatio, many simulations/challenges\n",
    "   - If Intuitive (score > 3): High intuitiveModeRatio, many concepts/patterns explored\n",
    "   - If Balanced: Moderate values for both\n",
    "   \n",
    "   Visual/Verbal:\n",
    "   - If Visual (score < -3): High visualModeRatio, many diagrams/wireframes viewed\n",
    "   - If Verbal (score > 3): High verbalModeRatio, much text read/summaries created\n",
    "   - If Balanced: Moderate values for both\n",
    "   \n",
    "   Sequential/Global:\n",
    "   - If Sequential (score < -3): High sequentialModeRatio, many steps/linear navigation\n",
    "   - If Global (score > 3): High globalModeRatio, many overviews/navigation jumps\n",
    "   - If Balanced: Moderate values for both\n",
    "\n",
    "3. ADD REALISTIC NOISE\n",
    "   - Add Gaussian noise (5% of value) to make data realistic\n",
    "   - Ensures features aren't perfectly predictive\n",
    "\n",
    "This creates a dataset where:\n",
    "- Features are causally related to labels (realistic)\n",
    "- Relationships are strong but not perfect (realistic noise)\n",
    "- ML models can learn the underlying patterns\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Current Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[all_features].values\n",
    "y_dict = {col: df[col].values for col in label_cols}\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train_dict, y_test_dict = {}, {}, {}, {}\n",
    "X_train_data, X_test_data = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_data)\n",
    "X_test_scaled = scaler.transform(X_test_data)\n",
    "\n",
    "print(f\"✅ Data prepared:\")\n",
    "print(f\"  Train: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"  Test: {X_test_scaled.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models and evaluate\n",
    "baseline_results = {}\n",
    "\n",
    "for label in label_cols:\n",
    "    # Split labels\n",
    "    y_train, y_test = train_test_split(y_dict[label], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train baseline XGBoost\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    baseline_results[label] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Accuracy': r2 * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  MAE: {mae:.3f}\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  R²: {r2:.3f} ({r2*100:.1f}%)\")\n",
    "\n",
    "# Overall performance\n",
    "avg_r2 = np.mean([r['R²'] for r in baseline_results.values()])\n",
    "print(f\"\\n🎯 Average R²: {avg_r2:.3f} ({avg_r2*100:.1f}%)\")\n",
    "print(f\"\\n⚠️ PROBLEM: Current accuracy is {avg_r2*100:.1f}%, target is 96%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Problem Diagnosis\n",
    "\n",
    "### Why is accuracy low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\n🔍 PROBLEM DIAGNOSIS\\n\" + \"=\"*60)\n",
    "\n",
    "Current Issues:\n",
    "\n",
    "1. ❌ INSUFFICIENT DATA\n",
    "   - Current: 500 samples\n",
    "   - Recommended: 2000-5000 samples for 96% accuracy\n",
    "   - With 24 features, need more data to learn patterns\n",
    "\n",
    "2. ❌ SIMPLE FEATURES\n",
    "   - Current: 24 basic behavioral features\n",
    "   - Missing: Interaction features, polynomial features, time-based features\n",
    "   - Need feature engineering to capture complex patterns\n",
    "\n",
    "3. ❌ BASIC HYPERPARAMETERS\n",
    "   - Using default XGBoost parameters\n",
    "   - Need hyperparameter tuning (GridSearch/RandomSearch)\n",
    "   - Can improve 5-10% with proper tuning\n",
    "\n",
    "4. ❌ NO ENSEMBLE\n",
    "   - Using single XGBoost model\n",
    "   - Ensemble methods (stacking, voting) can boost accuracy\n",
    "   - Combine XGBoost + Random Forest + Neural Network\n",
    "\n",
    "Solutions to implement:\n",
    "✅ Increase dataset to 2000+ samples\n",
    "✅ Add engineered features (interactions, polynomials)\n",
    "✅ Hyperparameter tuning with GridSearchCV\n",
    "✅ Create ensemble model\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
